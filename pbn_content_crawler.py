# PBN ì½˜í…ì¸  í¬ë¡¤ëŸ¬ ë° ë§í¬ ë¹Œë”
# ëª¨ë“  PBN ì‚¬ì´íŠ¸ì˜ ë¸”ë¡œê·¸ ê¸€ ì œëª©ê³¼ URLì„ ìˆ˜ì§‘í•˜ê³  ìœ ì‚¬ë„ ê¸°ë°˜ ë§í¬ ìƒì„±

import os
import json
import time
import requests
from datetime import datetime
from typing import List, Dict, Any, Tuple, Optional
from urllib.parse import urljoin, urlparse
import sqlite3
import re
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
try:
    from sentence_transformers import SentenceTransformer
    from sklearn.metrics.pairwise import cosine_similarity
    import numpy as np

    SIMILARITY_AVAILABLE = True
except ImportError:
    print("âš ï¸ sentence-transformers, scikit-learnì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("pip install sentence-transformers scikit-learn numpy ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
    SIMILARITY_AVAILABLE = False

from controlDB import get_all_pbn_sites


@dataclass
class PBNPost:
    """PBN í¬ìŠ¤íŠ¸ ë°ì´í„° í´ë˜ìŠ¤"""

    site_id: int
    site_url: str
    post_id: int
    title: str
    url: str
    excerpt: str
    date_published: str
    word_count: int = 0
    categories: List[str] = None
    tags: List[str] = None


class PBNContentCrawler:
    """PBN ì‚¬ì´íŠ¸ë“¤ì˜ ì½˜í…ì¸ ë¥¼ í¬ë¡¤ë§í•˜ê³  ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self, db_path: str = "controlDB.db"):
        """
        PBN ì½˜í…ì¸  í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”

        Args:
            db_path: SQLite ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ê²½ë¡œ
        """
        self.db_path = db_path
        self.session = self._create_session()
        self.lock = threading.Lock()

        # í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ëª¨ë¸ ì´ˆê¸°í™” (ê°€ëŠ¥í•œ ê²½ìš°)
        self.similarity_model = None
        if SIMILARITY_AVAILABLE:
            try:
                print("ğŸ¤– í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ëª¨ë¸ ë¡œë”© ì¤‘...")
                # í•œêµ­ì–´ì— ì í•©í•œ ë‹¤êµ­ì–´ ëª¨ë¸ ì‚¬ìš©
                self.similarity_model = SentenceTransformer(
                    "paraphrase-multilingual-MiniLM-L12-v2"
                )
                print("âœ… í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸ ìœ ì‚¬ë„ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                self.similarity_model = None

        # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
        self._init_database()

        print(f"ğŸ—„ï¸ PBN ì½˜í…ì¸  í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ (DB: {db_path})")

    def _create_session(self) -> requests.Session:
        """HTTP ì„¸ì…˜ ìƒì„± (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
        session = requests.Session()

        # ì¬ì‹œë„ ì „ëµ ì„¤ì •
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
        )

        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("http://", adapter)
        session.mount("https://", adapter)

        # ê¸°ë³¸ í—¤ë” ì„¤ì •
        session.headers.update(
            {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
            }
        )

        return session

    def _init_database(self):
        """SQLite ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()

            # PBN í¬ìŠ¤íŠ¸ í…Œì´ë¸” ìƒì„±
            cursor.execute(
                """
                CREATE TABLE IF NOT EXISTS pbn_posts (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    site_id INTEGER NOT NULL,
                    site_url TEXT NOT NULL,
                    post_id INTEGER NOT NULL,
                    title TEXT NOT NULL,
                    url TEXT NOT NULL UNIQUE,
                    excerpt TEXT,
                    date_published TEXT,
                    word_count INTEGER DEFAULT 0,
                    categories TEXT,  -- JSON í˜•íƒœë¡œ ì €ì¥
                    tags TEXT,        -- JSON í˜•íƒœë¡œ ì €ì¥
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """
            )

            # í¬ë¡¤ë§ ë¡œê·¸ í…Œì´ë¸”
            cursor.execute(
                """
                CREATE TABLE IF NOT EXISTS crawl_logs (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    site_id INTEGER NOT NULL,
                    site_url TEXT NOT NULL,
                    total_posts INTEGER DEFAULT 0,
                    successful_posts INTEGER DEFAULT 0,
                    failed_posts INTEGER DEFAULT 0,
                    crawl_duration REAL DEFAULT 0,
                    status TEXT DEFAULT 'pending',
                    error_message TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """
            )

            # ì¸ë±ìŠ¤ ìƒì„±
            cursor.execute(
                "CREATE INDEX IF NOT EXISTS idx_site_id ON pbn_posts(site_id)"
            )
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_title ON pbn_posts(title)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_url ON pbn_posts(url)")

            conn.commit()

        print("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")

    def get_wordpress_posts(
        self, site_url: str, site_id: int, max_pages: int = 50
    ) -> List[PBNPost]:
        """
        ì›Œë“œí”„ë ˆìŠ¤ ì‚¬ì´íŠ¸ì—ì„œ í¬ìŠ¤íŠ¸ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.

        Args:
            site_url: ì‚¬ì´íŠ¸ URL
            site_id: ì‚¬ì´íŠ¸ ID
            max_pages: ìµœëŒ€ í˜ì´ì§€ ìˆ˜

        Returns:
            PBNPost ê°ì²´ ë¦¬ìŠ¤íŠ¸
        """
        posts = []
        page = 1

        print(f"   ğŸ“„ í¬ìŠ¤íŠ¸ ìˆ˜ì§‘ ì‹œì‘: {site_url}")

        while page <= max_pages:
            try:
                # WordPress REST API ì—”ë“œí¬ì¸íŠ¸
                api_url = f"{site_url.rstrip('/')}/wp-json/wp/v2/posts"
                params = {
                    "page": page,
                    "per_page": 100,  # í•œ ë²ˆì— ë§ì´ ê°€ì ¸ì˜¤ê¸°
                    "status": "publish",
                    "_fields": "id,title,link,excerpt,date,categories,tags,content",  # í•„ìš”í•œ í•„ë“œë§Œ
                }

                response = self.session.get(api_url, params=params, timeout=30)

                if response.status_code == 404:
                    print(f"   âš ï¸ REST APIë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {site_url}")
                    break
                elif response.status_code != 200:
                    print(f"   âŒ API ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
                    break

                page_posts = response.json()

                if not page_posts:  # ë” ì´ìƒ í¬ìŠ¤íŠ¸ê°€ ì—†ìŒ
                    break

                for post_data in page_posts:
                    try:
                        # HTML íƒœê·¸ ì œê±° í•¨ìˆ˜
                        def clean_html(text):
                            if not text:
                                return ""
                            clean = re.sub("<.*?>", "", str(text))
                            return clean.strip()

                        # ì œëª© ì •ë¦¬
                        title = clean_html(
                            post_data.get("title", {}).get("rendered", "")
                        )
                        if not title:
                            continue

                        # ë³¸ë¬¸ì—ì„œ ë‹¨ì–´ ìˆ˜ ê³„ì‚° (ëŒ€ëµì )
                        content = clean_html(
                            post_data.get("content", {}).get("rendered", "")
                        )
                        word_count = len(content.split()) if content else 0

                        post = PBNPost(
                            site_id=site_id,
                            site_url=site_url,
                            post_id=post_data.get("id", 0),
                            title=title,
                            url=post_data.get("link", ""),
                            excerpt=clean_html(
                                post_data.get("excerpt", {}).get("rendered", "")
                            ),
                            date_published=post_data.get("date", ""),
                            word_count=word_count,
                            categories=[],  # ì¹´í…Œê³ ë¦¬ ì •ë³´ëŠ” ë³„ë„ API í˜¸ì¶œ í•„ìš”
                            tags=[],  # íƒœê·¸ ì •ë³´ë„ ë³„ë„ API í˜¸ì¶œ í•„ìš”
                        )

                        posts.append(post)

                    except Exception as e:
                        print(f"   âš ï¸ í¬ìŠ¤íŠ¸ íŒŒì‹± ì˜¤ë¥˜: {e}")
                        continue

                print(f"   ğŸ“„ í˜ì´ì§€ {page} ì™„ë£Œ: {len(page_posts)}ê°œ í¬ìŠ¤íŠ¸")
                page += 1

                # ìš”ì²­ ê°„ê²© (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                time.sleep(0.5)

            except Exception as e:
                print(f"   âŒ í˜ì´ì§€ {page} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                break

        print(f"   âœ… ì´ {len(posts)}ê°œ í¬ìŠ¤íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ: {site_url}")
        return posts

    def save_posts_to_db(self, posts: List[PBNPost]) -> int:
        """í¬ìŠ¤íŠ¸ë“¤ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        if not posts:
            return 0

        saved_count = 0

        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()

            for post in posts:
                try:
                    cursor.execute(
                        """
                        INSERT OR REPLACE INTO pbn_posts 
                        (site_id, site_url, post_id, title, url, excerpt, date_published, word_count, categories, tags, updated_at)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP)
                    """,
                        (
                            post.site_id,
                            post.site_url,
                            post.post_id,
                            post.title,
                            post.url,
                            post.excerpt,
                            post.date_published,
                            post.word_count,
                            json.dumps(post.categories or [], ensure_ascii=False),
                            json.dumps(post.tags or [], ensure_ascii=False),
                        ),
                    )
                    saved_count += 1
                except sqlite3.IntegrityError:
                    # URLì´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ê²½ìš° (ì¤‘ë³µ)
                    continue
                except Exception as e:
                    print(f"   âš ï¸ í¬ìŠ¤íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
                    continue

            conn.commit()

        return saved_count

    def log_crawl_result(
        self,
        site_id: int,
        site_url: str,
        total_posts: int,
        successful_posts: int,
        duration: float,
        status: str = "completed",
        error_message: str = None,
    ):
        """í¬ë¡¤ë§ ê²°ê³¼ë¥¼ ë¡œê·¸ì— ê¸°ë¡"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute(
                """
                INSERT INTO crawl_logs 
                (site_id, site_url, total_posts, successful_posts, failed_posts, crawl_duration, status, error_message)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """,
                (
                    site_id,
                    site_url,
                    total_posts,
                    successful_posts,
                    total_posts - successful_posts,
                    duration,
                    status,
                    error_message,
                ),
            )
            conn.commit()

    def crawl_single_site(self, site_data: Tuple) -> Dict[str, Any]:
        """ë‹¨ì¼ PBN ì‚¬ì´íŠ¸ í¬ë¡¤ë§"""
        site_id, site_url, username, password, app_password = site_data
        start_time = time.time()

        try:
            print(f"ğŸ•·ï¸ í¬ë¡¤ë§ ì‹œì‘: {site_url}")

            # í¬ìŠ¤íŠ¸ ìˆ˜ì§‘
            posts = self.get_wordpress_posts(site_url, site_id)

            # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            saved_count = self.save_posts_to_db(posts)

            duration = time.time() - start_time

            # ë¡œê·¸ ê¸°ë¡
            self.log_crawl_result(site_id, site_url, len(posts), saved_count, duration)

            result = {
                "site_id": site_id,
                "site_url": site_url,
                "total_posts": len(posts),
                "saved_posts": saved_count,
                "duration": duration,
                "status": "success",
            }

            print(
                f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: {site_url} ({saved_count}/{len(posts)} ì €ì¥, {duration:.1f}ì´ˆ)"
            )
            return result

        except Exception as e:
            duration = time.time() - start_time
            error_msg = str(e)

            # ì—ëŸ¬ ë¡œê·¸ ê¸°ë¡
            self.log_crawl_result(
                site_id, site_url, 0, 0, duration, "failed", error_msg
            )

            print(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {site_url} - {error_msg}")
            return {
                "site_id": site_id,
                "site_url": site_url,
                "total_posts": 0,
                "saved_posts": 0,
                "duration": duration,
                "status": "failed",
                "error": error_msg,
            }

    def crawl_all_pbn_sites(self, max_workers: int = 5) -> Dict[str, Any]:
        """ëª¨ë“  PBN ì‚¬ì´íŠ¸ë¥¼ ë³‘ë ¬ë¡œ í¬ë¡¤ë§"""
        print("ğŸš€ ëª¨ë“  PBN ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì‹œì‘")
        print("=" * 50)

        # PBN ì‚¬ì´íŠ¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        pbn_sites = get_all_pbn_sites()

        if not pbn_sites:
            print("âŒ PBN ì‚¬ì´íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {"total_sites": 0, "results": []}

        print(f"ğŸ“Š ì´ {len(pbn_sites)}ê°œì˜ PBN ì‚¬ì´íŠ¸ ë°œê²¬")

        # ë³‘ë ¬ í¬ë¡¤ë§ ì‹¤í–‰
        results = []
        total_posts = 0
        total_saved = 0
        successful_sites = 0

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # ëª¨ë“  ì‚¬ì´íŠ¸ì— ëŒ€í•´ í¬ë¡¤ë§ ì‘ì—… ì œì¶œ
            future_to_site = {
                executor.submit(self.crawl_single_site, site): site
                for site in pbn_sites
            }

            # ì™„ë£Œëœ ì‘ì—…ë“¤ ì²˜ë¦¬
            for future in as_completed(future_to_site):
                site = future_to_site[future]
                try:
                    result = future.result()
                    results.append(result)

                    total_posts += result["total_posts"]
                    total_saved += result["saved_posts"]

                    if result["status"] == "success":
                        successful_sites += 1

                except Exception as e:
                    print(f"âŒ ì‚¬ì´íŠ¸ ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {site[1]} - {e}")
                    results.append(
                        {
                            "site_id": site[0],
                            "site_url": site[1],
                            "total_posts": 0,
                            "saved_posts": 0,
                            "status": "exception",
                            "error": str(e),
                        }
                    )

        # ìµœì¢… ê²°ê³¼ ì¶œë ¥
        print("\n" + "=" * 50)
        print("ğŸ‰ PBN ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì™„ë£Œ!")
        print(f"ğŸ“Š ì²˜ë¦¬ëœ ì‚¬ì´íŠ¸: {len(pbn_sites)}ê°œ")
        print(f"âœ… ì„±ê³µí•œ ì‚¬ì´íŠ¸: {successful_sites}ê°œ")
        print(f"âŒ ì‹¤íŒ¨í•œ ì‚¬ì´íŠ¸: {len(pbn_sites) - successful_sites}ê°œ")
        print(f"ğŸ“„ ì´ ìˆ˜ì§‘ëœ í¬ìŠ¤íŠ¸: {total_posts:,}ê°œ")
        print(f"ğŸ’¾ ì €ì¥ëœ í¬ìŠ¤íŠ¸: {total_saved:,}ê°œ")

        return {
            "total_sites": len(pbn_sites),
            "successful_sites": successful_sites,
            "total_posts": total_posts,
            "saved_posts": total_saved,
            "results": results,
        }

    def get_all_posts_from_db(self) -> List[Dict[str, Any]]:
        """ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ëª¨ë“  í¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute(
                """
                SELECT site_id, site_url, post_id, title, url, excerpt, date_published, word_count
                FROM pbn_posts
                ORDER BY date_published DESC
            """
            )

            posts = []
            for row in cursor.fetchall():
                posts.append(
                    {
                        "site_id": row[0],
                        "site_url": row[1],
                        "post_id": row[2],
                        "title": row[3],
                        "url": row[4],
                        "excerpt": row[5],
                        "date_published": row[6],
                        "word_count": row[7],
                    }
                )

            return posts

    def find_similar_posts(
        self, keywords: List[str], limit: int = 10, min_similarity: float = 0.3
    ) -> List[Dict[str, Any]]:
        """
        í‚¤ì›Œë“œì™€ ìœ ì‚¬í•œ ì œëª©ì„ ê°€ì§„ í¬ìŠ¤íŠ¸ë“¤ì„ ì°¾ìŠµë‹ˆë‹¤.

        Args:
            keywords: ê²€ìƒ‰í•  í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            limit: ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜
            min_similarity: ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’

        Returns:
            ìœ ì‚¬í•œ í¬ìŠ¤íŠ¸ë“¤ì˜ ë¦¬ìŠ¤íŠ¸ (ìœ ì‚¬ë„ ì ìˆ˜ í¬í•¨)
        """
        if not self.similarity_model:
            print("âš ï¸ ìœ ì‚¬ë„ ëª¨ë¸ì´ ì—†ì–´ì„œ í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            return self._find_similar_posts_keyword_matching(keywords, limit)

        try:
            # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ëª¨ë“  í¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
            posts = self.get_all_posts_from_db()

            if not posts:
                return []

            # í‚¤ì›Œë“œ ì¡°í•© ìƒì„±
            search_text = " ".join(keywords)

            # ì œëª©ë“¤ ì¶”ì¶œ
            titles = [post["title"] for post in posts]

            # ì„ë² ë”© ìƒì„±
            print(f"ğŸ” {len(titles)}ê°œ í¬ìŠ¤íŠ¸ ì œëª©ì—ì„œ ìœ ì‚¬ë„ ê²€ì‚¬ ì¤‘...")
            search_embedding = self.similarity_model.encode([search_text])
            title_embeddings = self.similarity_model.encode(titles)

            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarities = cosine_similarity(search_embedding, title_embeddings)[0]

            # ìœ ì‚¬ë„ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
            similar_indices = np.argsort(similarities)[::-1]

            results = []
            for idx in similar_indices:
                similarity_score = similarities[idx]

                if similarity_score < min_similarity:
                    break

                if len(results) >= limit:
                    break

                post = posts[idx]
                post["similarity_score"] = float(similarity_score)
                results.append(post)

            # ìµœì‹  ê¸€ ìš°ì„  ì •ë ¬ (ë‚ ì§œ ê¸°ì¤€) - ê°œì„ ëœ ë²„ì „
            from datetime import datetime

            def safe_date_sort(post):
                """ì•ˆì „í•œ ë‚ ì§œ ì •ë ¬ì„ ìœ„í•œ í•¨ìˆ˜"""
                try:
                    # ISO í˜•ì‹ ë‚ ì§œ íŒŒì‹±
                    date_str = post.get("date_published", "")
                    if date_str:
                        return datetime.fromisoformat(date_str.replace("Z", "+00:00"))
                    else:
                        # ë‚ ì§œê°€ ì—†ìœ¼ë©´ ë§¤ìš° ì˜¤ë˜ëœ ê²ƒìœ¼ë¡œ ì²˜ë¦¬
                        return datetime.min
                except (ValueError, TypeError):
                    # ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì˜¤ë˜ëœ ê²ƒìœ¼ë¡œ ì²˜ë¦¬
                    return datetime.min

            # ìµœì‹  ê¸€ ìš°ì„ ìœ¼ë¡œ ì •ë ¬ (ë‚ ì§œ ë‚´ë¦¼ì°¨ìˆœ)
            results.sort(key=safe_date_sort, reverse=True)

            # ìµœì‹  ê¸€ ì •ë³´ ì¶œë ¥
            if results:
                latest_date = safe_date_sort(results[0])
                oldest_date = safe_date_sort(results[-1])
                print(
                    f"ğŸ“… ê²€ìƒ‰ëœ í¬ìŠ¤íŠ¸ ë‚ ì§œ ë²”ìœ„: {oldest_date.strftime('%Y-%m-%d')} ~ {latest_date.strftime('%Y-%m-%d')}"
                )

            print(f"âœ… {len(results)}ê°œì˜ ìœ ì‚¬í•œ í¬ìŠ¤íŠ¸ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤. (ìµœì‹  ê¸€ ìš°ì„ )")
            return results

        except Exception as e:
            print(f"âŒ ìœ ì‚¬ë„ ê²€ì‚¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return self._find_similar_posts_keyword_matching(keywords, limit)

    def _find_similar_posts_keyword_matching(
        self, keywords: List[str], limit: int
    ) -> List[Dict[str, Any]]:
        """í‚¤ì›Œë“œ ë§¤ì¹­ ë°©ì‹ìœ¼ë¡œ ìœ ì‚¬í•œ í¬ìŠ¤íŠ¸ ì°¾ê¸° (ë°±ì—… ë°©ë²•)"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()

            # LIKE ì¿¼ë¦¬ë¡œ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ì œëª© ì°¾ê¸°
            conditions = []
            params = []

            for keyword in keywords:
                conditions.append("title LIKE ?")
                params.append(f"%{keyword}%")

            query = f"""
                SELECT site_id, site_url, post_id, title, url, excerpt, date_published, word_count
                FROM pbn_posts
                WHERE {" OR ".join(conditions)}
                ORDER BY word_count DESC
                LIMIT ?
            """

            params.append(limit)
            cursor.execute(query, params)

            results = []
            for row in cursor.fetchall():
                results.append(
                    {
                        "site_id": row[0],
                        "site_url": row[1],
                        "post_id": row[2],
                        "title": row[3],
                        "url": row[4],
                        "excerpt": row[5],
                        "date_published": row[6],
                        "word_count": row[7],
                        "similarity_score": 0.5,  # ê¸°ë³¸ ì ìˆ˜
                    }
                )

            return results

    def get_database_stats(self) -> Dict[str, Any]:
        """ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ ì •ë³´ ë°˜í™˜"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()

            # ì „ì²´ í¬ìŠ¤íŠ¸ ìˆ˜
            cursor.execute("SELECT COUNT(*) FROM pbn_posts")
            total_posts = cursor.fetchone()[0]

            # ì‚¬ì´íŠ¸ë³„ í¬ìŠ¤íŠ¸ ìˆ˜
            cursor.execute(
                """
                SELECT site_url, COUNT(*) as post_count
                FROM pbn_posts
                GROUP BY site_url
                ORDER BY post_count DESC
            """
            )
            site_stats = cursor.fetchall()

            # ìµœê·¼ í¬ë¡¤ë§ ë¡œê·¸
            cursor.execute(
                """
                SELECT site_url, total_posts, successful_posts, status, created_at
                FROM crawl_logs
                ORDER BY created_at DESC
                LIMIT 10
            """
            )
            recent_crawls = cursor.fetchall()

            return {
                "total_posts": total_posts,
                "total_sites": len(site_stats),
                "site_stats": site_stats,
                "recent_crawls": recent_crawls,
            }


# í…ŒìŠ¤íŠ¸ ë° ì‹¤í–‰ í•¨ìˆ˜ë“¤
def test_crawler():
    """í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("ğŸ§ª PBN ì½˜í…ì¸  í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ì‹œì‘")

    crawler = PBNContentCrawler()

    # ëª¨ë“  PBN ì‚¬ì´íŠ¸ í¬ë¡¤ë§
    results = crawler.crawl_all_pbn_sites(max_workers=3)  # ë™ì‹œ ì‹¤í–‰ ìˆ˜ ì œí•œ

    # í†µê³„ ì¶œë ¥
    stats = crawler.get_database_stats()
    print(f"\nğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ í†µê³„:")
    print(f"   ì´ í¬ìŠ¤íŠ¸: {stats['total_posts']:,}ê°œ")
    print(f"   ì´ ì‚¬ì´íŠ¸: {stats['total_sites']}ê°œ")

    # ìœ ì‚¬ë„ í…ŒìŠ¤íŠ¸
    test_keywords = ["SEO", "ë°±ë§í¬", "ê²€ìƒ‰ì—”ì§„ìµœì í™”"]
    similar_posts = crawler.find_similar_posts(test_keywords, limit=5)

    print(f"\nğŸ” '{', '.join(test_keywords)}' í‚¤ì›Œë“œì™€ ìœ ì‚¬í•œ í¬ìŠ¤íŠ¸:")
    for post in similar_posts:
        print(f"   ğŸ“„ {post['title']} (ìœ ì‚¬ë„: {post.get('similarity_score', 0):.3f})")
        print(f"      ğŸ”— {post['url']}")


if __name__ == "__main__":
    test_crawler()
