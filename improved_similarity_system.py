# ê°œì„ ëœ PBN ì½˜í…ì¸  ìœ ì‚¬ë„ ê²€ì‚¬ ì‹œìŠ¤í…œ
# íš¨ìœ¨ì ì¸ ë²¡í„° ì €ì¥ê³¼ ë¹ ë¥¸ ê²€ìƒ‰ì„ ìœ„í•œ ì‹œìŠ¤í…œ

import os
import json
import sqlite3
import pickle
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
import time
from dataclasses import dataclass

# í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
try:
    from sentence_transformers import SentenceTransformer
    from sklearn.metrics.pairwise import cosine_similarity
    import faiss

    ADVANCED_SIMILARITY_AVAILABLE = True
except ImportError:
    print("âš ï¸ ê³ ê¸‰ ìœ ì‚¬ë„ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("pip install sentence-transformers scikit-learn faiss-cpu numpy")
    ADVANCED_SIMILARITY_AVAILABLE = False


@dataclass
class PostEmbedding:
    """í¬ìŠ¤íŠ¸ ì„ë² ë”© ë°ì´í„° í´ë˜ìŠ¤"""

    post_id: int
    site_id: int
    title: str
    url: str
    embedding: np.ndarray
    word_count: int


class ImprovedSimilaritySystem:
    """ê°œì„ ëœ ìœ ì‚¬ë„ ê²€ì‚¬ ì‹œìŠ¤í…œ"""

    def __init__(
        self,
        db_path: str = "controlDB.db",
        embedding_cache_dir: str = "embedding_cache",
    ):
        """
        ê°œì„ ëœ ìœ ì‚¬ë„ ì‹œìŠ¤í…œ ì´ˆê¸°í™”

        Args:
            db_path: SQLite ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ
            embedding_cache_dir: ì„ë² ë”© ìºì‹œ ë””ë ‰í† ë¦¬
        """
        self.db_path = db_path
        self.embedding_cache_dir = Path(embedding_cache_dir)
        self.embedding_cache_dir.mkdir(exist_ok=True)

        # íŒŒì¼ ê²½ë¡œë“¤
        self.embeddings_file = self.embedding_cache_dir / "post_embeddings.pkl"
        self.faiss_index_file = self.embedding_cache_dir / "faiss_index.bin"
        self.metadata_file = self.embedding_cache_dir / "post_metadata.json"

        # ëª¨ë¸ ë° ì¸ë±ìŠ¤ ì´ˆê¸°í™”
        self.similarity_model = None
        self.faiss_index = None
        self.post_metadata = []

        if ADVANCED_SIMILARITY_AVAILABLE:
            self._initialize_model()
            self._load_or_create_index()

        print(f"ğŸ” ê°œì„ ëœ ìœ ì‚¬ë„ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")

    def _initialize_model(self):
        """ìœ ì‚¬ë„ ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            print("ğŸ¤– í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ëª¨ë¸ ë¡œë”© ì¤‘...")
            # í•œêµ­ì–´ì— ìµœì í™”ëœ ë‹¤êµ­ì–´ ëª¨ë¸ ì‚¬ìš©
            self.similarity_model = SentenceTransformer(
                "paraphrase-multilingual-MiniLM-L12-v2"
            )
            print("âœ… ìœ ì‚¬ë„ ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ ìœ ì‚¬ë„ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            self.similarity_model = None

    def _load_or_create_index(self):
        """FAISS ì¸ë±ìŠ¤ ë¡œë“œ ë˜ëŠ” ìƒì„±"""
        if (
            self.faiss_index_file.exists()
            and self.metadata_file.exists()
            and self.similarity_model
        ):

            try:
                # ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ
                print("ğŸ“‚ ê¸°ì¡´ FAISS ì¸ë±ìŠ¤ ë¡œë”© ì¤‘...")
                self.faiss_index = faiss.read_index(str(self.faiss_index_file))

                with open(self.metadata_file, "r", encoding="utf-8") as f:
                    self.post_metadata = json.load(f)

                print(f"âœ… FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: {len(self.post_metadata)}ê°œ í¬ìŠ¤íŠ¸")
                return
            except Exception as e:
                print(f"âš ï¸ ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")

        # ìƒˆë¡œìš´ ì¸ë±ìŠ¤ ìƒì„±
        print("ğŸ”¨ ìƒˆë¡œìš´ FAISS ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
        self._rebuild_index()

    def _rebuild_index(self):
        """FAISS ì¸ë±ìŠ¤ ì¬êµ¬ì„±"""
        if not self.similarity_model:
            print("âŒ ìœ ì‚¬ë„ ëª¨ë¸ì´ ì—†ì–´ì„œ ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ëª¨ë“  í¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        posts = self._get_all_posts_from_db()

        if not posts:
            print("âŒ ë°ì´í„°ë² ì´ìŠ¤ì— í¬ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        print(f"ğŸ“„ {len(posts)}ê°œ í¬ìŠ¤íŠ¸ì˜ ì„ë² ë”© ìƒì„± ì¤‘...")

        # ì œëª©ë“¤ ì¶”ì¶œ
        titles = [post["title"] for post in posts]

        # ë°°ì¹˜ë¡œ ì„ë² ë”© ìƒì„± (íš¨ìœ¨ì„± í–¥ìƒ)
        batch_size = 100
        all_embeddings = []

        for i in range(0, len(titles), batch_size):
            batch_titles = titles[i : i + batch_size]
            batch_embeddings = self.similarity_model.encode(
                batch_titles, batch_size=batch_size, show_progress_bar=True
            )
            all_embeddings.extend(batch_embeddings)

            if i % (batch_size * 5) == 0:  # 500ê°œë§ˆë‹¤ ì§„í–‰ìƒí™© ì¶œë ¥
                print(f"   ì§„í–‰ë¥ : {min(i+batch_size, len(titles))}/{len(titles)}")

        # NumPy ë°°ì—´ë¡œ ë³€í™˜
        embeddings_matrix = np.array(all_embeddings, dtype=np.float32)

        # FAISS ì¸ë±ìŠ¤ ìƒì„±
        dimension = embeddings_matrix.shape[1]

        # IndexFlatIP: ë‚´ì  ê¸°ë°˜ ìœ ì‚¬ë„ (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ì™€ ìœ ì‚¬)
        # ì •ê·œí™”ëœ ë²¡í„°ì— ëŒ€í•´ ë‚´ì  = ì½”ì‚¬ì¸ ìœ ì‚¬ë„
        self.faiss_index = faiss.IndexFlatIP(dimension)

        # ë²¡í„° ì •ê·œí™” (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ìœ„í•´)
        faiss.normalize_L2(embeddings_matrix)

        # ì¸ë±ìŠ¤ì— ë²¡í„° ì¶”ê°€
        self.faiss_index.add(embeddings_matrix)

        # ë©”íƒ€ë°ì´í„° ì¤€ë¹„
        self.post_metadata = []
        for i, post in enumerate(posts):
            self.post_metadata.append(
                {
                    "index": i,
                    "post_id": post["post_id"],
                    "site_id": post["site_id"],
                    "site_url": post["site_url"],
                    "title": post["title"],
                    "url": post["url"],
                    "excerpt": post.get("excerpt", ""),
                    "word_count": post.get("word_count", 0),
                    "date_published": post.get("date_published", ""),
                }
            )

        # ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„° ì €ì¥
        self._save_index()

        print(f"âœ… FAISS ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ: {len(self.post_metadata)}ê°œ í¬ìŠ¤íŠ¸")

    def _save_index(self):
        """FAISS ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„° ì €ì¥"""
        try:
            # FAISS ì¸ë±ìŠ¤ ì €ì¥
            faiss.write_index(self.faiss_index, str(self.faiss_index_file))

            # ë©”íƒ€ë°ì´í„° ì €ì¥
            with open(self.metadata_file, "w", encoding="utf-8") as f:
                json.dump(self.post_metadata, f, ensure_ascii=False, indent=2)

            print("ğŸ’¾ ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ ì¸ë±ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")

    def _get_all_posts_from_db(self) -> List[Dict[str, Any]]:
        """ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ëª¨ë“  í¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute(
                """
                SELECT site_id, site_url, post_id, title, url, excerpt, 
                       date_published, word_count
                FROM pbn_posts
                ORDER BY date_published DESC
            """
            )

            posts = []
            for row in cursor.fetchall():
                posts.append(
                    {
                        "site_id": row[0],
                        "site_url": row[1],
                        "post_id": row[2],
                        "title": row[3],
                        "url": row[4],
                        "excerpt": row[5],
                        "date_published": row[6],
                        "word_count": row[7],
                    }
                )

            return posts

    def find_similar_posts_fast(
        self,
        keywords: List[str],
        limit: int = 10,
        min_similarity: float = 0.3,
        random_selection: bool = True,
    ) -> List[Dict[str, Any]]:
        """
        ë¹ ë¥¸ ìœ ì‚¬ë„ ê²€ìƒ‰ (FAISS ì¸ë±ìŠ¤ ì‚¬ìš©) - ëœë¤ ì„ íƒ ì˜µì…˜ í¬í•¨

        Args:
            keywords: ê²€ìƒ‰í•  í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            limit: ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜
            min_similarity: ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’
            random_selection: ëœë¤ ì„ íƒ ì—¬ë¶€ (True: ìƒìœ„ ê·¸ë£¹ì—ì„œ ëœë¤, False: ìƒìœ„ ìˆœì„œëŒ€ë¡œ)

        Returns:
            ìœ ì‚¬í•œ í¬ìŠ¤íŠ¸ë“¤ì˜ ë¦¬ìŠ¤íŠ¸ (ìœ ì‚¬ë„ ì ìˆ˜ í¬í•¨)
        """
        if not self.faiss_index or not self.similarity_model:
            print("âš ï¸ FAISS ì¸ë±ìŠ¤ë‚˜ ìœ ì‚¬ë„ ëª¨ë¸ì´ ì—†ì–´ì„œ ê¸°ë³¸ ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return self._find_similar_posts_basic(keywords, limit)

        try:
            # í‚¤ì›Œë“œ ì¡°í•© ìƒì„±
            search_text = " ".join(keywords)

            # ê²€ìƒ‰ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding = self.similarity_model.encode([search_text])
            query_embedding = query_embedding.astype(np.float32)

            # ë²¡í„° ì •ê·œí™”
            faiss.normalize_L2(query_embedding)

            # FAISSë¡œ ë¹ ë¥¸ ê²€ìƒ‰ ìˆ˜í–‰
            # ëœë¤ ì„ íƒì„ ìœ„í•´ ë” ë§ì€ í›„ë³´ ê²€ìƒ‰
            search_limit = min(
                limit * 5 if random_selection else limit * 3, len(self.post_metadata)
            )
            similarities, indices = self.faiss_index.search(
                query_embedding, search_limit
            )

            # ê²°ê³¼ ì²˜ë¦¬
            candidates = []
            for i, (similarity, idx) in enumerate(zip(similarities[0], indices[0])):
                if similarity < min_similarity:
                    continue

                metadata = self.post_metadata[idx]
                result = {
                    "site_id": metadata["site_id"],
                    "site_url": metadata["site_url"],
                    "post_id": metadata["post_id"],
                    "title": metadata["title"],
                    "url": metadata["url"],
                    "excerpt": metadata["excerpt"],
                    "date_published": metadata["date_published"],
                    "word_count": metadata["word_count"],
                    "similarity_score": float(similarity),
                }
                candidates.append(result)

            # ëœë¤ ì„ íƒ ë˜ëŠ” ìƒìœ„ ì„ íƒ
            if random_selection and len(candidates) > limit:
                import random

                # ìƒìœ„ 50% ë˜ëŠ” ìµœì†Œ limit*2ê°œ ì¤‘ì—ì„œ ì„ íƒ
                top_group_size = max(limit * 2, len(candidates) // 2)
                top_candidates = candidates[:top_group_size]

                # ëœë¤ìœ¼ë¡œ limitê°œ ì„ íƒ
                selected = random.sample(
                    top_candidates, min(limit, len(top_candidates))
                )

                print(
                    f"ğŸ² {len(candidates)}ê°œ í›„ë³´ ì¤‘ ìƒìœ„ {top_group_size}ê°œì—ì„œ ëœë¤ ì„ íƒ: {len(selected)}ê°œ"
                )
                return selected
            else:
                results = candidates[:limit]
                print(f"ğŸš€ FAISS ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ìœ ì‚¬ í¬ìŠ¤íŠ¸ ë°œê²¬")
                return results

        except Exception as e:
            print(f"âŒ FAISS ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return self._find_similar_posts_basic(keywords, limit)

    def _find_similar_posts_basic(
        self, keywords: List[str], limit: int
    ) -> List[Dict[str, Any]]:
        """ê¸°ë³¸ í‚¤ì›Œë“œ ë§¤ì¹­ ë°©ì‹ (ë°±ì—…)"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()

            # LIKE ì¿¼ë¦¬ë¡œ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ì œëª© ì°¾ê¸°
            conditions = []
            params = []

            for keyword in keywords:
                conditions.append("title LIKE ?")
                params.append(f"%{keyword}%")

            query = f"""
                SELECT site_id, site_url, post_id, title, url, excerpt, 
                       date_published, word_count
                FROM pbn_posts
                WHERE {" OR ".join(conditions)}
                ORDER BY word_count DESC
                LIMIT ?
            """

            params.append(limit)
            cursor.execute(query, params)

            results = []
            for row in cursor.fetchall():
                results.append(
                    {
                        "site_id": row[0],
                        "site_url": row[1],
                        "post_id": row[2],
                        "title": row[3],
                        "url": row[4],
                        "excerpt": row[5],
                        "date_published": row[6],
                        "word_count": row[7],
                        "similarity_score": 0.5,  # ê¸°ë³¸ ì ìˆ˜
                    }
                )

            return results

    def update_index_with_new_posts(self):
        """ìƒˆë¡œìš´ í¬ìŠ¤íŠ¸ê°€ ì¶”ê°€ëœ ê²½ìš° ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸"""
        print("ğŸ”„ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ í™•ì¸ ì¤‘...")

        # í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ì˜ í¬ìŠ¤íŠ¸ ìˆ˜ í™•ì¸
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT COUNT(*) FROM pbn_posts")
            current_count = cursor.fetchone()[0]

        # ì¸ë±ìŠ¤ì˜ í¬ìŠ¤íŠ¸ ìˆ˜ì™€ ë¹„êµ
        indexed_count = len(self.post_metadata) if self.post_metadata else 0

        if current_count > indexed_count:
            print(f"ğŸ“ˆ ìƒˆë¡œìš´ í¬ìŠ¤íŠ¸ ë°œê²¬: {current_count - indexed_count}ê°œ")
            print("ğŸ”¨ ì¸ë±ìŠ¤ ì¬êµ¬ì„± ì¤‘...")
            self._rebuild_index()
        else:
            print("âœ… ì¸ë±ìŠ¤ê°€ ìµœì‹  ìƒíƒœì…ë‹ˆë‹¤.")

    def get_index_stats(self) -> Dict[str, Any]:
        """ì¸ë±ìŠ¤ í†µê³„ ì •ë³´ ë°˜í™˜"""
        stats = {
            "total_posts": len(self.post_metadata) if self.post_metadata else 0,
            "index_exists": self.faiss_index is not None,
            "model_loaded": self.similarity_model is not None,
            "embedding_dimension": self.faiss_index.d if self.faiss_index else 0,
            "cache_dir": str(self.embedding_cache_dir),
            "index_file_size": 0,
            "metadata_file_size": 0,
        }

        # íŒŒì¼ í¬ê¸° ì •ë³´
        if self.faiss_index_file.exists():
            stats["index_file_size"] = self.faiss_index_file.stat().st_size

        if self.metadata_file.exists():
            stats["metadata_file_size"] = self.metadata_file.stat().st_size

        # ì¸ë±ìŠ¤ í¬ê¸° (MB)
        stats["index_size_mb"] = stats["index_file_size"] / (1024 * 1024)

        return stats


# ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def performance_comparison_test():
    """ê¸°ì¡´ ë°©ì‹ê³¼ ê°œì„ ëœ ë°©ì‹ì˜ ì„±ëŠ¥ ë¹„êµ"""
    print("âš¡ ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 50)

    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    improved_system = ImprovedSimilaritySystem()

    # í…ŒìŠ¤íŠ¸ í‚¤ì›Œë“œ
    test_keywords = ["SEO", "ë°±ë§í¬", "ê²€ìƒ‰ì—”ì§„ìµœì í™”"]

    # FAISS ë°©ì‹ í…ŒìŠ¤íŠ¸
    if improved_system.faiss_index:
        print("ğŸš€ FAISS ì¸ë±ìŠ¤ ë°©ì‹ í…ŒìŠ¤íŠ¸...")
        start_time = time.time()
        faiss_results = improved_system.find_similar_posts_fast(test_keywords, limit=10)
        faiss_duration = time.time() - start_time
        print(f"   ê²°ê³¼: {len(faiss_results)}ê°œ, ì†Œìš”ì‹œê°„: {faiss_duration:.3f}ì´ˆ")

    # ê¸°ë³¸ ë°©ì‹ í…ŒìŠ¤íŠ¸
    print("ğŸ“ ê¸°ë³¸ í‚¤ì›Œë“œ ë§¤ì¹­ ë°©ì‹ í…ŒìŠ¤íŠ¸...")
    start_time = time.time()
    basic_results = improved_system._find_similar_posts_basic(test_keywords, limit=10)
    basic_duration = time.time() - start_time
    print(f"   ê²°ê³¼: {len(basic_results)}ê°œ, ì†Œìš”ì‹œê°„: {basic_duration:.3f}ì´ˆ")

    # ì„±ëŠ¥ ê°œì„  ë¹„ìœ¨ ê³„ì‚°
    if improved_system.faiss_index and basic_duration > 0:
        improvement = (basic_duration - faiss_duration) / basic_duration * 100
        print(f"\nâš¡ ì„±ëŠ¥ ê°œì„ : {improvement:.1f}% ë¹ ë¦„")


if __name__ == "__main__":
    # ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
    system = ImprovedSimilaritySystem()

    # í†µê³„ ì¶œë ¥
    stats = system.get_index_stats()
    print(f"\nğŸ“Š ì‹œìŠ¤í…œ í†µê³„:")
    print(f"   ì´ í¬ìŠ¤íŠ¸: {stats['total_posts']:,}ê°œ")
    print(f"   FAISS ì¸ë±ìŠ¤: {'âœ…' if stats['index_exists'] else 'âŒ'}")
    print(f"   ìœ ì‚¬ë„ ëª¨ë¸: {'âœ…' if stats['model_loaded'] else 'âŒ'}")

    # ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸
    if stats["total_posts"] > 0:
        performance_comparison_test()
