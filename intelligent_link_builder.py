# ì§€ëŠ¥í˜• ë§í¬ ë¹Œë” ì‹œìŠ¤í…œ
# ì½˜í…ì¸  ë‚´ì— ë‚´ë¶€ë§í¬ì™€ ì™¸ë¶€ë§í¬ë¥¼ ìë™ìœ¼ë¡œ ì‚½ì…

import re
import random
from typing import List, Dict, Any, Tuple, Optional
from dataclasses import dataclass
from pbn_content_crawler import PBNContentCrawler
from improved_similarity_system import ImprovedSimilaritySystem


@dataclass
class LinkCandidate:
    """ë§í¬ í›„ë³´ ë°ì´í„° í´ë˜ìŠ¤"""

    text: str  # ì•µì»¤ í…ìŠ¤íŠ¸
    url: str  # ë§í¬ URL
    link_type: str  # 'internal' ë˜ëŠ” 'external'
    position: int  # ì½˜í…ì¸  ë‚´ ìœ„ì¹˜
    confidence: float  # ì‹ ë¢°ë„ ì ìˆ˜ (0-1)
    source: str  # ë§í¬ ì†ŒìŠ¤ ('client', 'pbn', 'external')


class IntelligentLinkBuilder:
    """ì§€ëŠ¥í˜• ë§í¬ ë¹Œë” í´ë˜ìŠ¤"""

    def __init__(self, crawler: PBNContentCrawler = None):
        """
        ë§í¬ ë¹Œë” ì´ˆê¸°í™”

        Args:
            crawler: PBN ì½˜í…ì¸  í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤
        """
        self.crawler = crawler or PBNContentCrawler()
        self.similarity_system = ImprovedSimilaritySystem()  # ğŸ§  AI ê¸°ë°˜ ìœ ì‚¬ë„ ì‹œìŠ¤í…œ

        # ë§í¬ ì‚½ì… ì„¤ì •
        self.config = {
            "external_link_probability": 1.0,  # ì™¸ë¶€ë§í¬(í´ë¼ì´ì–¸íŠ¸) 100% í™•ë¥ 
            "internal_link_probability": 1.0,  # ë‚´ë¶€ë§í¬ 100% í™•ë¥  (2~5ê°œ ëœë¤)
            "min_internal_links": 2,  # ìµœì†Œ ë‚´ë¶€ë§í¬ ìˆ˜ (1â†’2ë¡œ ì¦ê°€)
            "max_internal_links": 5,  # ìµœëŒ€ ë‚´ë¶€ë§í¬ ìˆ˜ (3â†’5ë¡œ ì¦ê°€)
            "max_external_links": 0,  # ì™¸ë¶€ë§í¬ ë¹„í™œì„±í™” (ì£¼ì„ì²˜ë¦¬)
            "min_similarity_score": 0.2,  # ìµœì†Œ ìœ ì‚¬ë„ ì ìˆ˜ (0.3â†’0.2ë¡œ ë‚®ì¶¤)
            "link_density_limit": 0.05,  # ë§í¬ ë°€ë„ ì œí•œ (3%â†’5%ë¡œ ì¦ê°€)
        }

        print("ğŸ”— ì§€ëŠ¥í˜• ë§í¬ ë¹Œë” ì´ˆê¸°í™” ì™„ë£Œ")

    def _calculate_similarity(self, word1: str, word2: str) -> float:
        """ë‘ ë‹¨ì–´ ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤ (ê°„ë‹¨í•œ Jaccard ìœ ì‚¬ë„)"""
        if not word1 or not word2:
            return 0.0

        # 2-gram ê¸°ë°˜ Jaccard ìœ ì‚¬ë„
        def get_ngrams(text, n=2):
            return set(text[i : i + n] for i in range(len(text) - n + 1))

        ngrams1 = get_ngrams(word1)
        ngrams2 = get_ngrams(word2)

        if not ngrams1 or not ngrams2:
            return 0.0

        intersection = len(ngrams1.intersection(ngrams2))
        union = len(ngrams1.union(ngrams2))

        return intersection / union if union > 0 else 0.0

    def _extract_additional_keywords(self, content: str) -> List[str]:
        """ì½˜í…ì¸ ì—ì„œ ì¶”ê°€ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤"""
        import re

        # HTML íƒœê·¸ ì œê±°
        clean_content = re.sub(r"<[^>]+>", " ", content)

        # íŠ¹ìˆ˜ë¬¸ì ì œê±°í•˜ê³  ë‹¨ì–´ ì¶”ì¶œ
        words = re.findall(r"\b[ê°€-í£]{2,}\b", clean_content)

        # ì œì™¸í•  ì¼ë°˜ì ì¸ ë‹¨ì–´ë“¤ (ë¶ˆìš©ì–´)
        stop_words = {
            "ê´€ë¦¬",
            "ê°„ë‹¨í•œ",
            "ê³µê³ ë¥¼",
            "ê²€ìƒ‰",
            "ì§€ì›",
            "ì°¾ëŠ”",
            "ì£¼ë§",
            "ë°©ë²•",
            "ê¸°ë²•",
            "ë„êµ¬",
            "ì›ë¦¬",
            "ì „ëµ",
            "ìš´ì˜",
            "ì„±ì¥",
            "ìˆ˜ìµì„±",
            "êµ¬í˜„",
            "ê°œë°œ",
            "ìµœì í™”",
            "í†µí•©",
            "ìë™í™”",
            "í•™ìŠµ",
            "í›ˆë ¨",
            "ì—­ëŸ‰",
            "ìš´ë™",
            "ì˜ì–‘",
            "ì›°ë¹™",
            "ì˜ˆë°©",
            "ë°©ë²•",
            "ê¸°ë²•",
            "ë„êµ¬",
            "ì›ë¦¬",
            "ì´ê²ƒ",
            "ì €ê²ƒ",
            "ê·¸ê²ƒ",
            "ì—¬ê¸°",
            "ì €ê¸°",
            "ê·¸ê¸°",
            "ë•Œë¬¸",
            "ìœ„í•´",
            "í†µí•´",
            "ëŒ€í•´",
            "ê´€ë ¨",
            "ë¹„í•´",
            "ë”°ë¼",
            "ì˜í•´",
            "ë¡œì„œ",
            "ìœ¼ë¡œ",
            "ì—ì„œ",
            "ì—ê²Œ",
            "í•œí…Œ",
            "ì²˜ëŸ¼",
            "ê°™ì´",
            "ë³´ë‹¤",
            "ë§ì´",
            "ì ê²Œ",
            "ì˜ëª»",
            "ë°”ë¥´ê²Œ",
            "ì˜¬ë°”ë¥´ê²Œ",
            "ì •í™•íˆ",
            "ëª…í™•íˆ",
            "í™•ì‹¤íˆ",
            "ë¶„ëª…íˆ",
            "ìì„¸íˆ",
            "ê°„ë‹¨íˆ",
            "ì‰½ê²Œ",
            "ì–´ë µê²Œ",
            "ë¹ ë¥´ê²Œ",
            "ì²œì²œíˆ",
            "ì¡°ê¸ˆì”©",
            "ì ì ",
            "ì ì°¨",
            "ì°¨ì¸°",
            "ì„œì„œíˆ",
            "ê¸‰ê²©íˆ",
            "ê¸‰ì†íˆ",
            "ëŠë¦¬ê²Œ",
            "ë¹¨ë¦¬",
        }

        # ë¹ˆë„ìˆ˜ ê³„ì‚° (ë¶ˆìš©ì–´ ì œì™¸)
        word_freq = {}
        for word in words:
            if (
                len(word) >= 2 and word not in stop_words
            ):  # 2ê¸€ì ì´ìƒì´ê³  ë¶ˆìš©ì–´ê°€ ì•„ë‹Œ ê²½ìš°ë§Œ
                word_freq[word] = word_freq.get(word, 0) + 1

        # ë¹ˆë„ìˆ˜ 3 ì´ìƒì¸ ë‹¨ì–´ë“¤ì„ í‚¤ì›Œë“œë¡œ ì„ íƒ (ìµœëŒ€ 5ê°œë¡œ ì œí•œ)
        additional_keywords = [
            word
            for word, freq in sorted(
                word_freq.items(), key=lambda x: x[1], reverse=True
            )
            if freq >= 3  # ë¹ˆë„ìˆ˜ ê¸°ì¤€ì„ 3ìœ¼ë¡œ ë†’ì„
        ][
            :5
        ]  # ìµœëŒ€ 5ê°œë¡œ ì œí•œ

        return additional_keywords

    def is_excluded_section(self, content: str, position: int) -> bool:
        """
        í•´ë‹¹ ìœ„ì¹˜ê°€ ë§í¬ ì‚½ì… ì œì™¸ ì˜ì—­ì¸ì§€ í™•ì¸
        ì œëª©, ëª©ì°¨, ìš©ì–´ì •ë¦¬ ì„¹ì…˜ë§Œ ì œì™¸í•˜ê³  ì¼ë°˜ ë³¸ë¬¸ì€ í—ˆìš©

        Args:
            content: ì „ì²´ ì½˜í…ì¸ 
            position: í™•ì¸í•  ìœ„ì¹˜

        Returns:
            ì œì™¸ ì˜ì—­ì´ë©´ True, ì•„ë‹ˆë©´ False
        """
        # ìœ„ì¹˜ ì•ë’¤ë¡œ 100ìì”© í™•ì¸ (ë²”ìœ„ ì¶•ì†Œ)
        start = max(0, position - 100)
        end = min(len(content), position + 100)
        context = content[start:end]

        # 1. ì œëª© íƒœê·¸ ë‚´ë¶€ í™•ì¸ (h1~h6 íƒœê·¸ ì•ˆì˜ í…ìŠ¤íŠ¸) - ë” ê´€ëŒ€í•˜ê²Œ
        title_pattern = r"<h[1-6][^>]*>.*?</h[1-6]>"
        title_matches = list(
            re.finditer(title_pattern, context, re.IGNORECASE | re.DOTALL)
        )
        for match in title_matches:
            if match.start() <= position - start <= match.end():
                # H2 íƒœê·¸ëŠ” í—ˆìš© (ì„¹ì…˜ ì œëª©ì´ë¯€ë¡œ)
                if "<h2" in match.group().lower():
                    print(f"      âœ… H2 íƒœê·¸ í—ˆìš©: {match.group()[:50]}...")
                    continue
                print(f"      ğŸ” ì œëª© íƒœê·¸ ë‚´ë¶€ ê°ì§€: {match.group()[:50]}...")
                return True

        # 2. ëª©ì°¨ ê´€ë ¨ íƒœê·¸ í™•ì¸ (ol, ul, li íƒœê·¸)
        list_patterns = [
            r"<ol[^>]*>.*?</ol>",  # ìˆœì„œìˆëŠ” ëª©ë¡
            r"<ul[^>]*>.*?</ul>",  # ìˆœì„œì—†ëŠ” ëª©ë¡
            r"<li[^>]*>.*?</li>",  # ëª©ë¡ í•­ëª©
        ]
        for pattern in list_patterns:
            matches = list(re.finditer(pattern, context, re.IGNORECASE | re.DOTALL))
            for match in matches:
                if match.start() <= position - start <= match.end():
                    print(f"      ğŸ” ëª©ë¡ íƒœê·¸ ë‚´ë¶€ ê°ì§€: {match.group()[:50]}...")
                    return True

        # 3. ìš©ì–´ì •ë¦¬ ê´€ë ¨ íƒœê·¸ í™•ì¸ (dl, dt, dd)
        definition_patterns = [
            r"<dl[^>]*>.*?</dl>",  # ì •ì˜ ëª©ë¡
            r"<dt[^>]*>.*?</dt>",  # ì •ì˜ ìš©ì–´
            r"<dd[^>]*>.*?</dd>",  # ì •ì˜ ì„¤ëª…
        ]
        for pattern in definition_patterns:
            matches = list(re.finditer(pattern, context, re.IGNORECASE | re.DOTALL))
            for match in matches:
                if match.start() <= position - start <= match.end():
                    print(f"      ğŸ” ì •ì˜ íƒœê·¸ ë‚´ë¶€ ê°ì§€: {match.group()[:50]}...")
                    return True

        # 4. íŠ¹ë³„í•œ IDë‚˜ í´ë˜ìŠ¤ í™•ì¸
        special_patterns = [
            r'id="[^"]*toc[^"]*"',  # ëª©ì°¨ ê´€ë ¨ ID
            r'id="[^"]*terms[^"]*"',  # ìš©ì–´ì •ë¦¬ ê´€ë ¨ ID
            r'class="[^"]*toc[^"]*"',  # ëª©ì°¨ ê´€ë ¨ í´ë˜ìŠ¤
            r'class="[^"]*terms[^"]*"',  # ìš©ì–´ì •ë¦¬ ê´€ë ¨ í´ë˜ìŠ¤
        ]
        for pattern in special_patterns:
            if re.search(pattern, context, re.IGNORECASE):
                print(f"      ğŸ” íŠ¹ë³„ ID/í´ë˜ìŠ¤ ê°ì§€: {pattern}")
                return True

        # 5. ë„¤ë¹„ê²Œì´ì…˜ íƒœê·¸ í™•ì¸
        if re.search(r"<nav[^>]*>", context, re.IGNORECASE):
            print(f"      ğŸ” ë„¤ë¹„ê²Œì´ì…˜ íƒœê·¸ ê°ì§€")
            return True

        # 6. <p> íƒœê·¸ ë‚´ë¶€ëŠ” í—ˆìš© (ì¼ë°˜ ë³¸ë¬¸)
        if re.search(r"<p[^>]*>", context, re.IGNORECASE):
            print(f"      âœ… <p> íƒœê·¸ ë‚´ë¶€ ë³¸ë¬¸ ì˜ì—­ (í—ˆìš©)")
            return False

        # 7. ê¸°íƒ€ ì¼ë°˜ ë³¸ë¬¸ ì˜ì—­ë„ í—ˆìš©
        print(f"      âœ… ì¼ë°˜ ë³¸ë¬¸ ì˜ì—­ìœ¼ë¡œ íŒë‹¨ (í—ˆìš©)")
        return False

    def _is_inside_title_tag(self, content: str, position: int) -> bool:
        """ì œëª© íƒœê·¸ ë‚´ë¶€ì¸ì§€ í™•ì¸"""
        start = max(0, position - 100)
        end = min(len(content), position + 100)
        context = content[start:end]

        title_pattern = r"<h[1-6][^>]*>.*?</h[1-6]>"
        matches = list(re.finditer(title_pattern, context, re.IGNORECASE | re.DOTALL))
        for match in matches:
            if match.start() <= position - start <= match.end():
                return True
        return False

    def _is_inside_list_tag(self, content: str, position: int) -> bool:
        """ëª©ë¡ íƒœê·¸ ë‚´ë¶€ì¸ì§€ í™•ì¸"""
        start = max(0, position - 100)
        end = min(len(content), position + 100)
        context = content[start:end]

        list_patterns = [
            r"<ol[^>]*>.*?</ol>",
            r"<ul[^>]*>.*?</ul>",
            r"<li[^>]*>.*?</li>",
        ]
        for pattern in list_patterns:
            matches = list(re.finditer(pattern, context, re.IGNORECASE | re.DOTALL))
            for match in matches:
                if match.start() <= position - start <= match.end():
                    return True
        return False

    def _is_inside_paragraph_tag(self, content: str, position: int) -> bool:
        """<p> íƒœê·¸ ë‚´ë¶€ì¸ì§€ í™•ì¸"""
        start = max(0, position - 100)
        end = min(len(content), position + 100)
        context = content[start:end]

        p_pattern = r"<p[^>]*>.*?</p>"
        matches = list(re.finditer(p_pattern, context, re.IGNORECASE | re.DOTALL))
        for match in matches:
            if match.start() <= position - start <= match.end():
                return True
        return False

    def _has_existing_link(self, content: str, position: int, length: int) -> bool:
        """í•´ë‹¹ ìœ„ì¹˜ì— ì´ë¯¸ ë§í¬ê°€ ìˆëŠ”ì§€ í™•ì¸"""
        # ìœ„ì¹˜ ì•ë’¤ë¡œ 50ìì”© í™•ì¸
        start = max(0, position - 50)
        end = min(len(content), position + length + 50)
        context = content[start:end]

        # <a> íƒœê·¸ê°€ ìˆëŠ”ì§€ í™•ì¸
        if re.search(r"<a\s+[^>]*>", context, re.IGNORECASE):
            return True
        return False

    def extract_keywords_from_content(
        self,
        content: str,
        main_keyword: str,
        lsi_keywords: List[str] = None,
        longtail_keywords: List[str] = None,
    ) -> List[str]:
        """
        ì½˜í…ì¸ ì—ì„œ ë§í¬ ê°€ëŠ¥í•œ í‚¤ì›Œë“œë“¤ì„ ì¶”ì¶œ

        Args:
            content: ë¶„ì„í•  ì½˜í…ì¸ 
            main_keyword: ë©”ì¸ í‚¤ì›Œë“œ
            lsi_keywords: LSI í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            longtail_keywords: ë¡±í…Œì¼ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸

        Returns:
            ì¶”ì¶œëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        """
        all_keywords = [main_keyword]

        if lsi_keywords:
            all_keywords.extend(lsi_keywords)

        if longtail_keywords:
            all_keywords.extend(longtail_keywords)

        # ì½˜í…ì¸ ì— ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” í‚¤ì›Œë“œë§Œ í•„í„°ë§ (ë¶€ë¶„ ë§¤ì¹­ í¬í•¨)
        existing_keywords = []
        for keyword in all_keywords:
            # ì •í™•í•œ ë§¤ì¹­
            if keyword.lower() in content.lower():
                existing_keywords.append(keyword)
            # ë¶€ë¶„ ë§¤ì¹­ (í‚¤ì›Œë“œì˜ 70% ì´ìƒ ì¼ì¹˜)
            elif len(keyword) >= 4:  # 4ê¸€ì ì´ìƒì¸ í‚¤ì›Œë“œë§Œ
                for word in content.lower().split():
                    if (
                        len(word) >= 4
                        and self._calculate_similarity(keyword.lower(), word) >= 0.7
                    ):
                        existing_keywords.append(keyword)
                        break  # í•˜ë‚˜ë§Œ ì°¾ìœ¼ë©´ ì¶©ë¶„

        # ì¶”ê°€ í‚¤ì›Œë“œ ì¶”ì¶œ (ì½˜í…ì¸ ì—ì„œ ì§ì ‘ ì¶”ì¶œ)
        additional_keywords = self._extract_additional_keywords(content)
        existing_keywords.extend(additional_keywords)

        # ì¤‘ë³µ ì œê±°
        existing_keywords = list(set(existing_keywords))

        print(
            f"ğŸ“ ì½˜í…ì¸ ì—ì„œ {len(existing_keywords)}ê°œ í‚¤ì›Œë“œ ë°œê²¬: {existing_keywords}"
        )
        return existing_keywords

    def find_client_link_position(
        self, content: str, keyword: str
    ) -> Optional[Tuple[int, int, str]]:
        """
        í´ë¼ì´ì–¸íŠ¸ ë§í¬ë¥¼ ì‚½ì…í•  ìµœì ì˜ ìœ„ì¹˜ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
        ì œì™¸ ì˜ì—­ê³¼ HTML íƒœê·¸ ë‚´ë¶€ëŠ” í”¼í•©ë‹ˆë‹¤.

        Args:
            content: ê²€ìƒ‰í•  ì½˜í…ì¸ 
            keyword: ì•µì»¤í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©í•  í‚¤ì›Œë“œ

        Returns:
            (ì‹œì‘ìœ„ì¹˜, ëìœ„ì¹˜, ë§¤ì¹­ëœí…ìŠ¤íŠ¸) ë˜ëŠ” None
        """
        # 1ìˆœìœ„: ì •í™•í•œ í‚¤ì›Œë“œ ë§¤ì¹­ (HTML íƒœê·¸ ì™¸ë¶€ì—ì„œë§Œ)
        pattern = r"\b" + re.escape(keyword) + r"\b"
        matches = list(re.finditer(pattern, content, re.IGNORECASE))

        for match in matches:
            start, end = match.start(), match.end()
            matched_text = match.group()

            # ì œì™¸ ì˜ì—­ í™•ì¸
            if self.is_excluded_section(content, start):
                print(f"      âš ï¸ ì œì™¸ ì˜ì—­ì—ì„œ ë°œê²¬: '{matched_text}' (ê±´ë„ˆëœ€)")
                continue

            # HTML íƒœê·¸ ë‚´ë¶€ì¸ì§€ í™•ì¸
            before_text = content[max(0, start - 50) : start]
            after_text = content[end : end + 50]

            # íƒœê·¸ ë‚´ë¶€ì— ìˆëŠ”ì§€ í™•ì¸
            if self._is_inside_html_tag(before_text, after_text):
                print(f"      âš ï¸ HTML íƒœê·¸ ë‚´ë¶€ì—ì„œ ë°œê²¬: '{matched_text}' (ê±´ë„ˆëœ€)")
                continue

            print(f"      âœ… ì ì ˆí•œ ìœ„ì¹˜ ë°œê²¬: '{matched_text}' (ìœ„ì¹˜: {start})")
            return (start, end, matched_text)

        # 2ìˆœìœ„: í‚¤ì›Œë“œê°€ í¬í•¨ëœ êµ¬ë¬¸ ì°¾ê¸°
        keyword_parts = keyword.split()
        if len(keyword_parts) > 1:
            for part in keyword_parts:
                if len(part) > 2:  # ë„ˆë¬´ ì§§ì€ ë‹¨ì–´ ì œì™¸
                    pattern = r"\b" + re.escape(part) + r"\b"
                    matches = list(re.finditer(pattern, content, re.IGNORECASE))

                    for match in matches:
                        start, end = match.start(), match.end()
                        matched_text = match.group()

                        # ì œì™¸ ì˜ì—­ í™•ì¸
                        if self.is_excluded_section(content, start):
                            continue

                        # HTML íƒœê·¸ ë‚´ë¶€ì¸ì§€ í™•ì¸
                        before_text = content[max(0, start - 50) : start]
                        after_text = content[end : end + 50]

                        if self._is_inside_html_tag(before_text, after_text):
                            continue

                        print(
                            f"      âœ… ë¶€ë¶„ í‚¤ì›Œë“œ ìœ„ì¹˜ ë°œê²¬: '{matched_text}' (ìœ„ì¹˜: {start})"
                        )
                        return (start, end, matched_text)

        return None

    def _is_inside_html_tag(self, before_text: str, after_text: str) -> bool:
        """
        í•´ë‹¹ ìœ„ì¹˜ê°€ HTML íƒœê·¸ ë‚´ë¶€ì¸ì§€ í™•ì¸

        Args:
            before_text: ìœ„ì¹˜ ì´ì „ í…ìŠ¤íŠ¸
            after_text: ìœ„ì¹˜ ì´í›„ í…ìŠ¤íŠ¸

        Returns:
            HTML íƒœê·¸ ë‚´ë¶€ì´ë©´ True
        """
        # íƒœê·¸ ì‹œì‘ê³¼ ëì„ ì°¾ê¸°
        last_open_tag = before_text.rfind("<")
        last_close_tag = before_text.rfind(">")

        # íƒœê·¸ê°€ ì—´ë ¤ìˆê³  ë‹«íˆì§€ ì•Šì•˜ìœ¼ë©´ íƒœê·¸ ë‚´ë¶€
        if last_open_tag > last_close_tag:
            return True

        # ë‹¤ìŒ íƒœê·¸ê°€ ë‹«ëŠ” íƒœê·¸ì¸ì§€ í™•ì¸
        next_close_tag = after_text.find(">")
        next_open_tag = after_text.find("<")

        if next_close_tag != -1 and (
            next_open_tag == -1 or next_close_tag < next_open_tag
        ):
            return True

        return False

    def insert_client_link(
        self, content: str, keyword: str, client_url: str
    ) -> Tuple[str, bool]:
        """
        í´ë¼ì´ì–¸íŠ¸ ë§í¬ë¥¼ ì½˜í…ì¸ ì— ì‚½ì… (100% í™•ë¥ ) - ê°„ë‹¨í•˜ê³  í™•ì‹¤í•œ ë°©ì‹

        Args:
            content: ì›ë³¸ ì½˜í…ì¸ 
            keyword: ì•µì»¤í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ
            client_url: í´ë¼ì´ì–¸íŠ¸ ì‚¬ì´íŠ¸ URL

        Returns:
            (ìˆ˜ì •ëœ ì½˜í…ì¸ , ì„±ê³µì—¬ë¶€)
        """
        print(f"ğŸ¯ í´ë¼ì´ì–¸íŠ¸ ë§í¬ ì‚½ì… ì‹œë„: '{keyword}' -> {client_url}")

        # 1. í‚¤ì›Œë“œê°€ ë³¸ë¬¸ì— ìˆëŠ”ì§€ í™•ì¸
        if keyword.lower() not in content.lower():
            print(f"   ğŸ”§ í‚¤ì›Œë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê°•ì œ ì‚½ì…: ì½˜í…ì¸  ì‹œì‘ ë¶€ë¶„")
            link_html = (
                f'<a href="{client_url}" target="_blank" rel="noopener">{keyword}</a>'
            )
            modified_content = link_html + " " + content
            return modified_content, True

        # 2. ëª¨ë“  í‚¤ì›Œë“œ ìœ„ì¹˜ ì°¾ê¸°
        pattern = re.escape(keyword)
        matches = list(re.finditer(pattern, content, re.IGNORECASE))

        # 3. ê°€ì¥ ì í•©í•œ ìœ„ì¹˜ ì°¾ê¸° (ìš°ì„ ìˆœìœ„: <p> íƒœê·¸ > ì¼ë°˜ ë³¸ë¬¸)
        best_position = None
        best_priority = 0  # 0: ì œì™¸, 1: ì¼ë°˜ ë³¸ë¬¸, 2: <p> íƒœê·¸

        for match in matches:
            position = match.start()

            # ì œì™¸ ì¡°ê±´ í™•ì¸
            if self._is_inside_title_tag(content, position):
                print(
                    f"      ğŸ” ì œëª© íƒœê·¸ ë‚´ë¶€ ê°ì§€: {content[position-20:position+20]}..."
                )
                continue
            if self._is_inside_list_tag(content, position):
                print(
                    f"      ğŸ” ëª©ë¡ íƒœê·¸ ë‚´ë¶€ ê°ì§€: {content[position-20:position+20]}..."
                )
                continue
            if self._has_existing_link(content, position, len(keyword)):
                print(f"      âš ï¸ ì´ë¯¸ ë§í¬ê°€ ìˆëŠ” ìœ„ì¹˜: '{keyword}'")
                continue

            # ìš°ì„ ìˆœìœ„ ê²°ì •
            priority = 1  # ê¸°ë³¸: ì¼ë°˜ ë³¸ë¬¸
            if self._is_inside_paragraph_tag(content, position):
                priority = 2  # <p> íƒœê·¸ ë‚´ë¶€ê°€ ë” ìš°ì„ 

            if priority > best_priority:
                best_position = position
                best_priority = priority

        # 4. ê°€ì¥ ì í•©í•œ ìœ„ì¹˜ì— ë§í¬ ì‚½ì…
        if best_position is not None:
            # ë§í¬ HTML ìƒì„±
            link_html = (
                f'<a href="{client_url}" target="_blank" rel="noopener">{keyword}</a>'
            )

            # ì½˜í…ì¸ ì— ë§í¬ ì‚½ì…
            modified_content = (
                content[:best_position]
                + link_html
                + content[best_position + len(keyword) :]
            )

            if best_priority == 2:
                print(f"      âœ… <p> íƒœê·¸ ë‚´ë¶€ ë³¸ë¬¸: '{keyword}'")
            else:
                print(f"      âœ… ì¼ë°˜ ë³¸ë¬¸ ì˜ì—­: '{keyword}'")

            print(f"   âœ… í´ë¼ì´ì–¸íŠ¸ ë§í¬ ì‚½ì… ì„±ê³µ: '{keyword}'")
            return modified_content, True
        else:
            # ì í•©í•œ ìœ„ì¹˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš° ê°•ì œ ì‚½ì…
            print(f"   ğŸ”§ ì í•©í•œ ìœ„ì¹˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê°•ì œ ì‚½ì…: ì½˜í…ì¸  ì‹œì‘ ë¶€ë¶„")
            link_html = (
                f'<a href="{client_url}" target="_blank" rel="noopener">{keyword}</a>'
            )
            modified_content = link_html + " " + content
            return modified_content, True

    def find_internal_link_opportunities(
        self, content: str, keywords: List[str]
    ) -> List[Dict[str, Any]]:
        """
        ë‚´ë¶€ë§í¬ ì‚½ì… ê¸°íšŒë¥¼ ì°¾ìŠµë‹ˆë‹¤.

        Args:
            content: ë¶„ì„í•  ì½˜í…ì¸ 
            keywords: ê²€ìƒ‰í•  í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸

        Returns:
            ë‚´ë¶€ë§í¬ í›„ë³´ ë¦¬ìŠ¤íŠ¸
        """
        print(f"ğŸ” ë‚´ë¶€ë§í¬ ê¸°íšŒ íƒìƒ‰ ì¤‘... (í‚¤ì›Œë“œ: {len(keywords)}ê°œ)")

        # PBNì—ì„œ ìœ ì‚¬í•œ í¬ìŠ¤íŠ¸ ì°¾ê¸° (ê°„ë‹¨í•œ ë°©ì‹)
        similar_posts = self.similarity_system.find_similar_posts_fast(
            keywords,
            limit=9,  # ì¶©ë¶„í•œ í›„ë³´ í™•ë³´
            min_similarity=0.3,  # ë‚®ì€ ì„ê³„ê°’ìœ¼ë¡œ ë” ë§ì€ í›„ë³´ í™•ë³´
            random_selection=True,
        )

        if not similar_posts:
            print("   âŒ ìœ ì‚¬í•œ PBN í¬ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []

        print(f"   ğŸ“„ {len(similar_posts)}ê°œì˜ ìœ ì‚¬í•œ PBN í¬ìŠ¤íŠ¸ ë°œê²¬")

        # ë§í¬ í›„ë³´ ìƒì„±
        link_candidates = []

        for i, post in enumerate(similar_posts):
            # ì½˜í…ì¸ ì—ì„œ ì´ í¬ìŠ¤íŠ¸ì™€ ê´€ë ¨ëœ í‚¤ì›Œë“œ ì°¾ê¸°
            post_title = post["title"]
            post_url = post["url"]
            similarity_score = post.get("similarity_score", 0.5)

            print(f"   ğŸ” í¬ìŠ¤íŠ¸ {i+1} ë¶„ì„: {post_title[:50]}...")

            # ì›ë³¸ í‚¤ì›Œë“œë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì•µì»¤í…ìŠ¤íŠ¸ í›„ë³´ë¡œ ì‚¬ìš©
            potential_anchors = []

            # 1ìˆœìœ„: ì½˜í…ì¸ ì—ì„œ ë°œê²¬ëœ ì›ë³¸ í‚¤ì›Œë“œë“¤
            original_keywords_found = []
            for keyword in keywords:
                if keyword.lower() in content.lower():
                    potential_anchors.append(keyword)
                    original_keywords_found.append(keyword)

            print(f"      ğŸ¯ ì›ë³¸ í‚¤ì›Œë“œ ë°œê²¬: {original_keywords_found}")

            # 2ìˆœìœ„: PBN í¬ìŠ¤íŠ¸ ì œëª©ì—ì„œ ì¶”ì¶œí•œ ë‹¨ì–´ë“¤ (ì›ë³¸ í‚¤ì›Œë“œê°€ ë¶€ì¡±í•  ë•Œë§Œ)
            if len(potential_anchors) < 3:  # ì›ë³¸ í‚¤ì›Œë“œê°€ 3ê°œ ë¯¸ë§Œì¼ ë•Œë§Œ
                title_words = re.findall(r"\w+", post_title)
                additional_words = []
                for word in title_words:
                    if (
                        len(word) > 2
                        and word.lower() in content.lower()
                        and word not in potential_anchors
                    ):  # ì¤‘ë³µ ë°©ì§€
                        potential_anchors.append(word)
                        additional_words.append(word)
                print(f"      ğŸ“ ì œëª©ì—ì„œ ì¶”ê°€ ë°œê²¬: {additional_words}")

            print(f"      ğŸ“ ìµœì¢… ì•µì»¤í…ìŠ¤íŠ¸ í›„ë³´: {potential_anchors}")

            # ê°€ì¥ ì ì ˆí•œ ì•µì»¤í…ìŠ¤íŠ¸ ì„ íƒ (ë‹¤ì–‘ì„± ê³ ë ¤)
            if potential_anchors:
                # ì¤‘ë³µì„ í”¼í•˜ê¸° ìœ„í•´ ì´ë¯¸ ì‚¬ìš©ëœ ì•µì»¤í…ìŠ¤íŠ¸ ì œì™¸
                available_anchors = [
                    anchor
                    for anchor in potential_anchors
                    if not any(
                        candidate.get("anchor_text") == anchor
                        for candidate in link_candidates
                    )
                ]

                if not available_anchors:
                    # ëª¨ë“  í›„ë³´ê°€ ì´ë¯¸ ì‚¬ìš©ë¨ - ëœë¤ ì„ íƒ
                    available_anchors = potential_anchors

                # ê¸¸ì´ì™€ ê´€ë ¨ì„±ì„ ê³ ë ¤í•´ì„œ ì„ íƒ (ë‹¤ì–‘ì„± ìš°ì„ )
                if len(available_anchors) > 1:
                    # 2ê°œ ì´ìƒì´ë©´ ëœë¤ ì„ íƒìœ¼ë¡œ ë‹¤ì–‘ì„± í™•ë³´
                    best_anchor = random.choice(available_anchors)
                else:
                    best_anchor = available_anchors[0]

                print(
                    f"      ğŸ¯ ì„ íƒëœ ì•µì»¤í…ìŠ¤íŠ¸: '{best_anchor}' (í›„ë³´: {len(available_anchors)}ê°œ)"
                )

                # ì½˜í…ì¸ ì—ì„œ ìœ„ì¹˜ ì°¾ê¸°
                pattern = r"\b" + re.escape(best_anchor) + r"\b"
                match = re.search(pattern, content, re.IGNORECASE)

                if match:
                    position = match.start()
                    print(f"      ğŸ“ ìœ„ì¹˜ ë°œê²¬: {position}")

                    # ì œì™¸ ì˜ì—­ í™•ì¸
                    if self.is_excluded_section(content, position):
                        print(f"      âš ï¸ ì œì™¸ ì˜ì—­ì´ë¯€ë¡œ ê±´ë„ˆëœ€: '{best_anchor}'")
                        continue

                    # ì´ë¯¸ ë§í¬ê°€ ìˆëŠ”ì§€ í™•ì¸
                    surrounding_text = content[
                        max(0, position - 20) : position + len(best_anchor) + 20
                    ]
                    if "<a " in surrounding_text and "</a>" in surrounding_text:
                        print(f"      âš ï¸ ì´ë¯¸ ë§í¬ê°€ ìˆëŠ” ìœ„ì¹˜: '{best_anchor}'")
                        continue

                    link_candidates.append(
                        {
                            "anchor_text": best_anchor,
                            "url": post_url,
                            "position": position,
                            "confidence": similarity_score,
                            "post_title": post_title,
                            "site_url": post["site_url"],
                        }
                    )
                    print(f"      âœ… ë§í¬ í›„ë³´ ì¶”ê°€: '{best_anchor}' -> {post_url}")
                else:
                    print(f"      âŒ ì½˜í…ì¸ ì—ì„œ ìœ„ì¹˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: '{best_anchor}'")
            else:
                print(f"      âŒ ì•µì»¤í…ìŠ¤íŠ¸ í›„ë³´ ì—†ìŒ")

        # ì‹ ë¢°ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        link_candidates.sort(key=lambda x: x["confidence"], reverse=True)

        print(f"   ğŸ”— {len(link_candidates)}ê°œì˜ ë‚´ë¶€ë§í¬ í›„ë³´ ìƒì„±")
        return link_candidates

    def find_internal_link_opportunities_simple(
        self, content: str, keywords: List[str], client_keyword: str = None
    ) -> List[Dict[str, Any]]:
        """
        ë‚´ë¶€ë§í¬ ì‚½ì… ê¸°íšŒë¥¼ ì°¾ìŠµë‹ˆë‹¤. (ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” ë¡œì§)

        Args:
            content: ë¶„ì„í•  ì½˜í…ì¸ 
            keywords: ê²€ìƒ‰í•  í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            client_keyword: í´ë¼ì´ì–¸íŠ¸ ë§í¬ì— ì‚¬ìš©ëœ í‚¤ì›Œë“œ (ì œì™¸ìš©)

        Returns:
            ë‚´ë¶€ë§í¬ í›„ë³´ ë¦¬ìŠ¤íŠ¸
        """
        print(f"ğŸ” ë‚´ë¶€ë§í¬ ê¸°íšŒ íƒìƒ‰ ì¤‘... (í‚¤ì›Œë“œ: {len(keywords)}ê°œ)")

        # 1. PBNì—ì„œ ìœ ì‚¬í•œ í¬ìŠ¤íŠ¸ ì°¾ê¸° (ë” ë§ì€ í›„ë³´, ë‚®ì€ ì„ê³„ê°’)
        similar_posts = self.similarity_system.find_similar_posts_fast(
            keywords,
            limit=50,  # í›„ë³´ í™•ë³´ (20â†’50ìœ¼ë¡œ ì¦ê°€)
            min_similarity=0.15,  # ì„ê³„ê°’ ë‚®ì¶¤ (0.3â†’0.15)
            random_selection=True,
        )

        if not similar_posts:
            print("   âŒ ìœ ì‚¬í•œ PBN í¬ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []

        print(f"   ğŸ“„ {len(similar_posts)}ê°œì˜ ìœ ì‚¬í•œ PBN í¬ìŠ¤íŠ¸ ë°œê²¬")

        # 2. ë‚´ë¶€ë§í¬ ê°œìˆ˜ ëœë¤ ì„ íƒ (2~5ê°œ)
        target_count = random.randint(2, 5)
        print(f"   ğŸ² ë‚´ë¶€ë§í¬ ëª©í‘œ ê°œìˆ˜: {target_count}ê°œ")

        # 3. PBN í¬ìŠ¤íŠ¸ì—ì„œ ëœë¤ìœ¼ë¡œ ì„ íƒ
        selected_posts = random.sample(
            similar_posts, min(target_count, len(similar_posts))
        )
        print(f"   ğŸ² ì„ íƒëœ PBN í¬ìŠ¤íŠ¸: {len(selected_posts)}ê°œ")

        # 4. í´ë¼ì´ì–¸íŠ¸ ë§í¬ì— ì‚¬ìš©ëœ í‚¤ì›Œë“œ ì œì™¸
        available_keywords = (
            [kw for kw in keywords if kw.lower() != client_keyword.lower()]
            if client_keyword
            else keywords
        )
        print(f"   ğŸ¯ ì‚¬ìš© ê°€ëŠ¥í•œ í‚¤ì›Œë“œ: {available_keywords}")

        if not available_keywords:
            print("   âŒ ì‚¬ìš© ê°€ëŠ¥í•œ í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []

        # 5. ê° í‚¤ì›Œë“œì— ëŒ€í•´ ë§í¬ í›„ë³´ ìƒì„±
        link_candidates = []

        for i, post in enumerate(selected_posts):
            post_title = post.get("title", "")
            post_url = post.get("url", "")

            print(f"   ğŸ” í¬ìŠ¤íŠ¸ {i+1} ë¶„ì„: {post_title[:50]}...")

            # ì‚¬ìš© ê°€ëŠ¥í•œ í‚¤ì›Œë“œ ì¤‘ì—ì„œ ë³¸ë¬¸ì— ìˆëŠ” ê²ƒë“¤ ì°¾ê¸° (ë¶€ë¶„ ë§¤ì¹­ í¬í•¨)
            valid_keywords = []
            for keyword in available_keywords:
                # ì •í™•í•œ ë§¤ì¹­
                if keyword.lower() in content.lower():
                    valid_keywords.append((keyword, 1.0))  # (í‚¤ì›Œë“œ, ìš°ì„ ìˆœìœ„)
                # ë¶€ë¶„ ë§¤ì¹­ (í‚¤ì›Œë“œì˜ 70% ì´ìƒ ì¼ì¹˜)
                elif len(keyword) >= 4:  # 4ê¸€ì ì´ìƒì¸ í‚¤ì›Œë“œë§Œ
                    for word in content.lower().split():
                        if (
                            len(word) >= 4
                            and self._calculate_similarity(keyword.lower(), word) >= 0.7
                        ):
                            valid_keywords.append(
                                (keyword, 0.8)
                            )  # ë¶€ë¶„ ë§¤ì¹­ì€ ë‚®ì€ ìš°ì„ ìˆœìœ„

            if not valid_keywords:
                print(f"      âŒ í¬ìŠ¤íŠ¸ {i+1}ì— ì‚¬ìš© ê°€ëŠ¥í•œ í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
                continue

            # ìš°ì„ ìˆœìœ„ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ê°€ì¥ ì í•©í•œ í‚¤ì›Œë“œ ì„ íƒ
            valid_keywords.sort(key=lambda x: x[1], reverse=True)
            selected_keyword = valid_keywords[0][0]
            print(f"      ğŸ¯ ì„ íƒëœ í‚¤ì›Œë“œ: '{selected_keyword}'")

            # í‚¤ì›Œë“œ ìœ„ì¹˜ ì°¾ê¸°
            pattern = re.escape(selected_keyword)
            matches = list(re.finditer(pattern, content, re.IGNORECASE))

            # ê°€ì¥ ì í•©í•œ ìœ„ì¹˜ ì°¾ê¸°
            best_position = None
            best_priority = 0

            for match in matches:
                position = match.start()

                # ì œì™¸ ì¡°ê±´ í™•ì¸
                if self._is_inside_title_tag(content, position):
                    continue
                if self._is_inside_list_tag(content, position):
                    continue
                if self._has_existing_link(content, position, len(selected_keyword)):
                    continue

                # ìš°ì„ ìˆœìœ„ ê²°ì •
                priority = 1  # ê¸°ë³¸: ì¼ë°˜ ë³¸ë¬¸
                if self._is_inside_paragraph_tag(content, position):
                    priority = 2  # <p> íƒœê·¸ ë‚´ë¶€ê°€ ë” ìš°ì„ 

                if priority > best_priority:
                    best_position = position
                    best_priority = priority

            # ë§í¬ í›„ë³´ ìƒì„±
            if best_position is not None:
                candidate = {
                    "anchor_text": selected_keyword,
                    "url": post_url,
                    "position": best_position,
                    "post_title": post_title,
                    "similarity_score": post.get("similarity_score", 0.5),
                }

                link_candidates.append(candidate)

                if best_priority == 2:
                    print(f"      âœ… <p> íƒœê·¸ ë‚´ë¶€ ë³¸ë¬¸: '{selected_keyword}'")
                else:
                    print(f"      âœ… ì¼ë°˜ ë³¸ë¬¸ ì˜ì—­: '{selected_keyword}'")

                print(
                    f"      ğŸ¯ ë‚´ë¶€ë§í¬ í›„ë³´ ìƒì„±: '{selected_keyword}' -> {post_url[:50]}..."
                )

        print(f"   ğŸ”— {len(link_candidates)}ê°œì˜ ë‚´ë¶€ë§í¬ í›„ë³´ ìƒì„±")
        return link_candidates

    def insert_internal_links(
        self, content: str, link_candidates: List[Dict[str, Any]], target_count: int = 3
    ) -> Tuple[str, List[Dict[str, Any]]]:
        """
        ë‚´ë¶€ë§í¬ë“¤ì„ ì½˜í…ì¸ ì— ì‚½ì…

        Args:
            content: ì›ë³¸ ì½˜í…ì¸ 
            link_candidates: ë§í¬ í›„ë³´ ë¦¬ìŠ¤íŠ¸
            target_count: ëª©í‘œ ë§í¬ ìˆ˜

        Returns:
            (ìˆ˜ì •ëœ ì½˜í…ì¸ , ì‚½ì…ëœ ë§í¬ ì •ë³´)
        """
        if not link_candidates:
            return content, []

        print(
            f"ğŸ”— ë‚´ë¶€ë§í¬ ì‚½ì… ì‹œì‘: {len(link_candidates)}ê°œ í›„ë³´ (ëª©í‘œ: {target_count}ê°œ)"
        )

        modified_content = content
        inserted_links = []
        offset = 0  # ì‚½ì…ìœ¼ë¡œ ì¸í•œ ìœ„ì¹˜ ì˜¤í”„ì…‹

        # ìœ„ì¹˜ ìˆœìœ¼ë¡œ ì •ë ¬ (ì•ì—ì„œë¶€í„° ì‚½ì…)
        candidates_by_position = sorted(link_candidates, key=lambda x: x["position"])

        for i, candidate in enumerate(candidates_by_position):
            if len(inserted_links) >= target_count:
                break

            anchor_text = candidate["anchor_text"]
            url = candidate["url"]
            position = candidate["position"] + offset

            # ì œì™¸ ì˜ì—­ í™•ì¸ (ë” ê´€ëŒ€í•˜ê²Œ)
            if self.is_excluded_section(modified_content, position):
                print(f"   âš ï¸ ì œì™¸ ì˜ì—­ì…ë‹ˆë‹¤ (ì œëª©/ëª©ì°¨/ìš©ì–´ì •ë¦¬): '{anchor_text}'")
                continue

            # ì´ë¯¸ ë§í¬ê°€ ìˆëŠ” ìœ„ì¹˜ì¸ì§€ í™•ì¸
            surrounding_text = modified_content[
                max(0, position - 20) : position + len(anchor_text) + 20
            ]
            if "<a " in surrounding_text and "</a>" in surrounding_text:
                print(f"   âš ï¸ ì´ë¯¸ ë§í¬ê°€ ìˆëŠ” ìœ„ì¹˜ ê±´ë„ˆëœ€: '{anchor_text}'")
                continue

            # ì •í™•í•œ ìœ„ì¹˜ì—ì„œ í…ìŠ¤íŠ¸ ì°¾ê¸° (ë” ë„“ì€ ë²”ìœ„)
            pattern = re.escape(anchor_text)
            match = re.search(
                pattern, modified_content[position : position + 100], re.IGNORECASE
            )

            if match:
                actual_start = position + match.start()
                actual_end = position + match.end()
                actual_text = modified_content[actual_start:actual_end]

                # ë§í¬ HTML ìƒì„±
                link_html = (
                    f'<a href="{url}" target="_blank" rel="noopener">{actual_text}</a>'
                )

                # ì½˜í…ì¸ ì— ë§í¬ ì‚½ì…
                modified_content = (
                    modified_content[:actual_start]
                    + link_html
                    + modified_content[actual_end:]
                )

                # ì˜¤í”„ì…‹ ì—…ë°ì´íŠ¸
                offset += len(link_html) - len(actual_text)

                # ì‚½ì…ëœ ë§í¬ ì •ë³´ ê¸°ë¡
                inserted_links.append(
                    {
                        "anchor_text": actual_text,
                        "url": url,
                        "post_title": candidate["post_title"],
                        "site_url": candidate.get(
                            "site_url", url
                        ),  # site_urlì´ ì—†ìœ¼ë©´ url ì‚¬ìš©
                        "confidence": candidate.get(
                            "similarity_score", 0.5
                        ),  # confidence ëŒ€ì‹  similarity_score ì‚¬ìš©
                    }
                )

                print(f"   âœ… ë‚´ë¶€ë§í¬ ì‚½ì…: '{actual_text}' -> {url}")

        print(f"ğŸ‰ ë‚´ë¶€ë§í¬ ì‚½ì… ì™„ë£Œ: {len(inserted_links)}ê°œ ì‚½ì…")
        return modified_content, inserted_links

    def add_additional_external_links(
        self, content: str, keywords: List[str]
    ) -> Tuple[str, List[Dict[str, Any]]]:
        """
        ì¶”ê°€ ì™¸ë¶€ë§í¬ ì‚½ì… (ê¶Œìœ„ìˆëŠ” ì‚¬ì´íŠ¸ë“¤)

        Args:
            content: ì›ë³¸ ì½˜í…ì¸ 
            keywords: í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸

        Returns:
            (ìˆ˜ì •ëœ ì½˜í…ì¸ , ì‚½ì…ëœ ë§í¬ ì •ë³´)
        """
        # ê¶Œìœ„ìˆëŠ” ì‚¬ì´íŠ¸ë“¤ (ì˜ˆì‹œ)
        authoritative_sites = [
            {"domain": "wikipedia.org", "name": "ìœ„í‚¤í”¼ë””ì•„"},
            {"domain": "naver.com", "name": "ë„¤ì´ë²„"},
            {"domain": "google.com", "name": "êµ¬ê¸€"},
            {"domain": "tistory.com", "name": "í‹°ìŠ¤í† ë¦¬"},
            {"domain": "brunch.co.kr", "name": "ë¸ŒëŸ°ì¹˜"},
        ]

        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë” ì •êµí•œ ë¡œì§ í•„ìš”
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ ì˜ˆì‹œë¡œ êµ¬í˜„

        modified_content = content
        inserted_links = []

        # í™•ë¥ ì ìœ¼ë¡œ 1-2ê°œì˜ ì™¸ë¶€ë§í¬ ì¶”ê°€
        if random.random() < 0.5 and keywords:  # 50% í™•ë¥ 
            selected_keyword = random.choice(keywords)
            selected_site = random.choice(authoritative_sites)

            # ê°€ìƒì˜ URL ìƒì„± (ì‹¤ì œë¡œëŠ” ê²€ìƒ‰ API ë“±ì„ ì‚¬ìš©í•´ì•¼ í•¨)
            external_url = (
                f"https://{selected_site['domain']}/search?q={selected_keyword}"
            )

            # í‚¤ì›Œë“œ ìœ„ì¹˜ ì°¾ê¸°
            pattern = r"\b" + re.escape(selected_keyword) + r"\b"
            matches = list(re.finditer(pattern, modified_content, re.IGNORECASE))

            if matches:
                # ë§ˆì§€ë§‰ ë§¤ì¹­ ìœ„ì¹˜ì— ë§í¬ ì‚½ì… (ì´ë¯¸ í´ë¼ì´ì–¸íŠ¸ ë§í¬ê°€ ìˆì„ ê°€ëŠ¥ì„±ì´ ë‚®ìŒ)
                match = matches[-1]
                start, end = match.start(), match.end()
                matched_text = match.group()

                # ì´ë¯¸ ë§í¬ê°€ ìˆëŠ”ì§€ í™•ì¸
                surrounding = modified_content[max(0, start - 10) : end + 10]
                if "<a " not in surrounding:
                    link_html = f'<a href="{external_url}" target="_blank" rel="noopener">{matched_text}</a>'
                    modified_content = (
                        modified_content[:start] + link_html + modified_content[end:]
                    )

                    inserted_links.append(
                        {
                            "anchor_text": matched_text,
                            "url": external_url,
                            "site_name": selected_site["name"],
                            "type": "external",
                        }
                    )

                    print(
                        f"   ğŸŒ ì™¸ë¶€ë§í¬ ì¶”ê°€: '{matched_text}' -> {selected_site['name']}"
                    )

        return modified_content, inserted_links

    def build_comprehensive_links(
        self,
        content: str,
        keyword: str,
        client_url: str,
        lsi_keywords: List[str] = None,
        longtail_keywords: List[str] = None,
    ) -> Dict[str, Any]:
        """
        ì¢…í•©ì ì¸ ë§í¬ ë¹Œë”© ìˆ˜í–‰

        Args:
            content: ì›ë³¸ ì½˜í…ì¸ 
            keyword: ë©”ì¸ í‚¤ì›Œë“œ
            client_url: í´ë¼ì´ì–¸íŠ¸ ì‚¬ì´íŠ¸ URL
            lsi_keywords: LSI í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            longtail_keywords: ë¡±í…Œì¼ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸

        Returns:
            ë§í¬ ë¹Œë”© ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        print("ğŸš€ ì¢…í•©ì ì¸ ë§í¬ ë¹Œë”© ì‹œì‘")
        print("=" * 50)

        modified_content = content
        link_report = {
            "client_link": None,
            "internal_links": [],
            "external_links": [],
            "total_links": 0,
            "success": False,
        }

        try:
            # 1. í´ë¼ì´ì–¸íŠ¸ ë§í¬ ì‚½ì… (100% í™•ë¥ )
            print("ğŸ¯ 1ë‹¨ê³„: í´ë¼ì´ì–¸íŠ¸ ë§í¬ ì‚½ì…")
            modified_content, client_success = self.insert_client_link(
                modified_content, keyword, client_url
            )

            if client_success:
                link_report["client_link"] = {
                    "keyword": keyword,
                    "url": client_url,
                    "type": "client",
                }
                print("   âœ… í´ë¼ì´ì–¸íŠ¸ ë§í¬ ì‚½ì… ì„±ê³µ")
            else:
                print("   âŒ í´ë¼ì´ì–¸íŠ¸ ë§í¬ ì‚½ì… ì‹¤íŒ¨")

            # 2. í‚¤ì›Œë“œ ì¶”ì¶œ
            print("\nğŸ” 2ë‹¨ê³„: ë§í¬ ê°€ëŠ¥í•œ í‚¤ì›Œë“œ ì¶”ì¶œ")
            all_keywords = self.extract_keywords_from_content(
                modified_content, keyword, lsi_keywords, longtail_keywords
            )

            # 3. ë‚´ë¶€ë§í¬ ì‚½ì…
            print("\nğŸ”— 3ë‹¨ê³„: ë‚´ë¶€ë§í¬ ì‚½ì…")
            if all_keywords:
                # 2~5ê°œ ëœë¤ ì„ íƒ
                target_count = random.randint(
                    self.config["min_internal_links"], self.config["max_internal_links"]
                )
                print(f"ğŸ² ë‚´ë¶€ë§í¬ ëª©í‘œ ê°œìˆ˜: {target_count}ê°œ (2~5ê°œ ì¤‘ ëœë¤ ì„ íƒ)")

                link_candidates = self.find_internal_link_opportunities_simple(
                    modified_content, all_keywords, keyword  # í´ë¼ì´ì–¸íŠ¸ í‚¤ì›Œë“œ ì „ë‹¬
                )

                if link_candidates:
                    modified_content, internal_links = self.insert_internal_links(
                        modified_content, link_candidates, target_count
                    )
                    link_report["internal_links"] = internal_links
                else:
                    print("   âŒ ë‚´ë¶€ë§í¬ í›„ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            # 4. ì¶”ê°€ ì™¸ë¶€ë§í¬ ì‚½ì… (ë¹„í™œì„±í™”)
            # print("\nğŸŒ 4ë‹¨ê³„: ì¶”ê°€ ì™¸ë¶€ë§í¬ ì‚½ì…")
            # modified_content, external_links = self.add_additional_external_links(
            #     modified_content, all_keywords
            # )
            # link_report["external_links"] = external_links
            link_report["external_links"] = []  # ì™¸ë¶€ë§í¬ ë¹„í™œì„±í™”

            # 5. ìµœì¢… ê²°ê³¼ ê³„ì‚°
            link_report["total_links"] = (
                (1 if link_report["client_link"] else 0)
                + len(link_report["internal_links"])
                + len(link_report["external_links"])
            )
            link_report["success"] = link_report["total_links"] > 0

            # ìµœì¢… ë³´ê³ ì„œ ì¶œë ¥
            print("\n" + "=" * 50)
            print("ğŸ‰ ë§í¬ ë¹Œë”© ì™„ë£Œ!")
            print(f"ğŸ“Š ì´ ì‚½ì…ëœ ë§í¬: {link_report['total_links']}ê°œ")
            print(f"   ğŸ¯ í´ë¼ì´ì–¸íŠ¸ ë§í¬: {1 if link_report['client_link'] else 0}ê°œ")
            print(f"   ğŸ”— ë‚´ë¶€ë§í¬: {len(link_report['internal_links'])}ê°œ")
            print(f"   ğŸŒ ì™¸ë¶€ë§í¬: {len(link_report['external_links'])}ê°œ")

            if link_report["internal_links"]:
                print("\nğŸ“‹ ì‚½ì…ëœ ë‚´ë¶€ë§í¬:")
                for link in link_report["internal_links"]:
                    print(f"   â€¢ '{link['anchor_text']}' -> {link['post_title']}")

            return {"content": modified_content, "report": link_report}

        except Exception as e:
            print(f"âŒ ë§í¬ ë¹Œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback

            traceback.print_exc()

            link_report["success"] = False
            link_report["error"] = str(e)

            return {"content": content, "report": link_report}  # ì›ë³¸ ì½˜í…ì¸  ë°˜í™˜


# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_link_builder():
    """ë§í¬ ë¹Œë” í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª ë§í¬ ë¹Œë” í…ŒìŠ¤íŠ¸ ì‹œì‘")

    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_content = """
    <h1>ë°±ë§í¬ í…ŒìŠ¤íŠ¸ì˜ ì¤‘ìš”ì„±</h1>
    <p>ë°±ë§í¬ëŠ” SEOì—ì„œ ë§¤ìš° ì¤‘ìš”í•œ ìš”ì†Œì…ë‹ˆë‹¤. ê²€ìƒ‰ì—”ì§„ìµœì í™”ë¥¼ ìœ„í•´ì„œëŠ” ê³ í’ˆì§ˆì˜ ë°±ë§í¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.</p>
    <p>ë°±ë§í¬ í…ŒìŠ¤íŠ¸ë¥¼ í†µí•´ ì›¹ì‚¬ì´íŠ¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>
    <h2>SEO ì „ëµ</h2>
    <p>íš¨ê³¼ì ì¸ SEO ì „ëµì—ëŠ” ë°±ë§í¬ êµ¬ì¶•ì´ í¬í•¨ë©ë‹ˆë‹¤.</p>
    """

    test_keyword = "ë°±ë§í¬ í…ŒìŠ¤íŠ¸"
    test_client_url = "https://example-client.com"
    test_lsi_keywords = ["SEO", "ê²€ìƒ‰ì—”ì§„ìµœì í™”", "ë§í¬ë¹Œë”©"]
    test_longtail_keywords = ["ë°±ë§í¬ í…ŒìŠ¤íŠ¸ ë°©ë²•", "SEO ë°±ë§í¬ ì „ëµ"]

    # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” (ì‹¤ì œ ë°ì´í„°ê°€ ìˆë‹¤ê³  ê°€ì •)
    crawler = PBNContentCrawler()
    link_builder = IntelligentLinkBuilder(crawler)

    # ë§í¬ ë¹Œë”© ì‹¤í–‰
    result = link_builder.build_comprehensive_links(
        test_content,
        test_keyword,
        test_client_url,
        test_lsi_keywords,
        test_longtail_keywords,
    )

    print("\nğŸ“„ ê²°ê³¼ ì½˜í…ì¸ :")
    print(result["content"])

    print("\nğŸ“Š ë§í¬ ë¹Œë”© ë³´ê³ ì„œ:")
    print(json.dumps(result["report"], indent=2, ensure_ascii=False))


if __name__ == "__main__":
    import json

    test_link_builder()
